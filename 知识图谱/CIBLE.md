- `nentity` \ `nrelation`：数据集中的实体数和关系数

- `train_triples`：训练集三元组，格式 `[(head_id,relation_id,tail_id),...]`

1、初始化模型 `KGE`，设置训练中迭代更新的模型参数，包括：

- **数据集中所有实体和关系**的嵌入向量矩阵：`entity_embedding`  \ `relation_embedding` 。
  - 形状为`(实体数\关系数，嵌入向量的维度)`
  - 初始化时控制所有嵌入向量的 2-范数 在 $[-\sqrt{6},\sqrt{6}]$ 区间
- 初始化 `IBLE` 模型，设置模型参数，包括：
  - 如果使用关系感知，关系感知嵌入层中，**每个关系对应的头实体和尾实体**的嵌入向量矩阵：`head_r` \ `tail_r`
    - 形状为：
      - 如果是对角矩阵嵌入diag：`(关系数，嵌入向量维度)`；
      - 如果是矩阵嵌入matrix：`(关系数，嵌入向量维度，嵌入向量维度)`
    - 初始化为控制所有嵌入向量的 2-范数 在 $[-\sqrt{6},\sqrt{6}]$ 区间

2、设置训练数据加载器，并将头实体和尾实体数据加载器合并成一个数据迭代器，以便在训练模型时循环遍历头实体和尾实体的数据集。

- 负采样方式：
  - 若是全采样，采样全部实体，则每一个正样本对应的负样本都是 `(0,1,...,nentity-1)` 。
  - 否则随机采样大小在 `[0,self.nentity)` 之间的 `negative_sample_size` 个负样本，且负样本与正样本 `(head,relation)\(relation,tail)` 组成的三元组不是真实三元组。

- 返回数据的形式：元组`(positive_sample, negative_sample, subsampling_weight, mode, labels)`
  - `positive_sample`：`(batch_size,3)`，每一行是正样本三元组的头实体、关系、尾实体的 ID。
  - `negative_sample`：`(batch_size, negative_sample_size)`，每一行都是负采样的实体。
  - `subsampling_weight`：`(batch_size，1)`，每一行是正样本三元组的负采样概率。
  - `mode`：表示这个 batch 是以头实体为基准还是以尾实体为基准。
  - `labels`：弃用。

3、设置优化器和学习率调度器：

- 优化器： 定义优化器为 `AdamW`，并设置优化器要优化的**模型参数**，设置**学习率**
- 学习率调度器：设置学习率，先使用 `warm up`，在使用指数衰减学习率调度程序，初始学习率为 `optimizer` 的学习率，衰减因子为`0.95`或`0.1`，具体取决于 warm_up_steps 是否被设置。

4、开始训练循环，训练目标是最小化正例得分与负例得分之间的差距：

- 基础模型使用翻译模型，例如`TransE`，分别计算正样本与负样本的得分。

- 另外采用`IBLE`模型，正负样本的得分都计算在一个所有目标节点的矩阵中：

  - 首先如果采用关系感知，则使用==关系感知嵌入层==，将**当前正样本源实体嵌入向量**以及所有实体向量，和**对应关系向量**进行乘法操作，将实体的嵌入和关系信息进行融合，得到嵌入的结果。

  - 使用余弦相似度时：

    - 先将所有实体向量和**当前正样本源实体向量**进行矩阵乘法，再通过==激活函数==（默认无，可设置为`Tanh`）进行激活，得到**当前所有正样本源实体向量**与其他每个实体向量之间的相似度/距离矩阵 `dis`。
    - 将 `dis` 矩阵通过一个图卷积层的聚合器，进行信息传递，得到边的得分 `edge_score` 。
    - 接着，依据**当前正样本三元组**中使用到的关系，将 `edge_score` 中无关系的边的得分置为 0。
    - 将处理后的 `edge_score` 边的矩阵通过另一个图卷积层的聚合器，进行信息传递，得到**当前每个目标实体向量**的得分。
    - 通过这种方式，我们可以将所有可以通过关系 r 连接的**正样本源实体向量**的相似度，首先传递给关系 r  对应的**边**，然后再传递给**目标实体**，得到目标实体的分数。

  - 如果使用欧氏距离，则与论文给出的公式一致。

    > *最后的得分也可以在通过一个 `MLP` ==多层感知器== 进行非线性变换。*

- 最终得分需要将模型的输出与 `IBLE` 损失函数的输出进行合并，公式为：
  $$
  ible\_score \times ible\_weight + sigmoid(origin\_score) \times (1-ible\_weight)
  $$

- 再将正样本得分和负样本得分拼接在一起，作为模型对当前批次样本的预测得分。
- 最后损失函数的选择：
  - 如果是采用最终得分与标签的交叉熵作为 `loss`：
    - 全采样则标签是**每个正样本目标节点实体**。
    - 部分采样则将标签设为全 0，表示所有样本都是负例，让神经网络去预测这些负例的概率，并计算损失。
  - 如果采用的是 `margin-based loss` ：
    - 将正负样本得分通过 `log-sigmoid` 函数，负样本对第一维取平均，正样本对第一维进行压缩。（一个正样本对应多个负样本，故多个负样本取平均）
    - 使用采样概率 `subsampling_weight` 对每个样本进行加权，以消除正负样本数量不均衡所带来的影响。
    - 最后将正负样本的损失加权求和并除以 2 得到总的损失 `loss`
