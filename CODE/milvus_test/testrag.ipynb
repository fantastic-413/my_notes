{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hybrid_search_collection']\n",
      "集合 hybrid_search_collection 已删除\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    utility,\n",
    "    FieldSchema, CollectionSchema, DataType,\n",
    "    Collection,\n",
    "    AnnSearchRequest, RRFRanker, WeightedRanker,\n",
    "    db\n",
    ")\n",
    "import numpy as np\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from transformers import pipeline\n",
    "import os\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "devices = [\"cuda:1\"]\n",
    "MILVUS_HOST = \"localhost\"\n",
    "MILVUS_PORT = \"19530\"\n",
    "TARGET_DB = \"test_db\"  # 自定义数据库名称\n",
    "COLLECTION_NAME=\"hybrid_search_collection\"\n",
    "EMBEDDING_MODEL_PATH = \"/disk3/lsp/models/BAAI/bge-m3\"\n",
    "\n",
    "# 连接 Milvus\n",
    "connections.connect(host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "\n",
    "# 创建数据库（如果不存在）\n",
    "if TARGET_DB not in db.list_database():\n",
    "    print(f\"数据库 {TARGET_DB} 不存在，创建数据库\")\n",
    "    db.create_database(TARGET_DB)\n",
    "# 删除数据库\n",
    "# db.drop_database(TARGET_DB)\n",
    "\n",
    "# 切换当前数据库\n",
    "db.using_database(TARGET_DB)\n",
    "print(\"当前数据库：\", TARGET_DB)\n",
    "# 列出当前所有collection\n",
    "print(\"当前数据库集合：\", utility.list_collections())\n",
    "\n",
    "if utility.has_collection(COLLECTION_NAME):\n",
    "    utility.drop_collection(COLLECTION_NAME)\n",
    "    print(f\"集合 {COLLECTION_NAME} 已删除\")\n",
    "# for collection_name in utility.list_collections():\n",
    "#     utility.drop_collection(collection_name)\n",
    "#     print(f\"Collection {collection_name} dropped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====================\n",
    "# 1. PDF文本解析与处理\n",
    "# ====================\n",
    "def extract_text_from_pdf(pdf_dir):\n",
    "    # 获取目录下所有pdf文件\n",
    "    pdf_paths = [os.path.join(pdf_dir, f) for f in os.listdir(pdf_dir) if f.endswith('.pdf')]\n",
    "    data = []\n",
    "    print(\"=====================================\")\n",
    "    print(\"PDF 文件数：\", len(pdf_paths))\n",
    "    # 逐个解析pdf文件\n",
    "    for pdf_file in pdf_paths:\n",
    "        print(f\"解析 PDF ：{pdf_file}\")\n",
    "        loader = PyMuPDFLoader(pdf_file)\n",
    "        data += loader.load()\n",
    "    print('数据总数: ', len(data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 2. 文本分割\n",
    "# ====================\n",
    "def split_documents(docs, chunk_size=1024, chunk_overlap=20):\n",
    "    \"\"\"使用LangChain的递归字符分割器\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"。\", \"！\", \"？\", \"；\"]\n",
    "    )\n",
    "    \n",
    "    return text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 3. 向量嵌入模型初始化\n",
    "# ====================\n",
    "def init_embedding_model(model_dir=\"/disk3/lsp/models/BAAI/bge-m3\", devices=[\"cuda:1\"]):\n",
    "    model = BGEM3FlagModel(model_dir, use_fp16=False, devices=devices)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 4. 读取 PDF 文档并分割\n",
    "# ====================\n",
    "def ingest_document(pdf_dir, batch_size=32):\n",
    "    # 从 PDF 中提取文本\n",
    "    docs = extract_text_from_pdf(pdf_dir)\n",
    "    \n",
    "    print('batch_size: ', batch_size)\n",
    "    chunks = []\n",
    "    for i in tqdm(range(0, len(docs), batch_size), desc=\"Splitting PDF into chunks\"):\n",
    "        batch_docs = docs[i:i+batch_size]\n",
    "        chunks.extend(split_documents(batch_docs))\n",
    "    print('chunks: ',len(chunks))\n",
    "        \n",
    "    \n",
    "    # chunks = split_documents(docs)\n",
    "    # print('chunks: ',len(chunks))\n",
    "\n",
    "    # 在 ingest_document 中增加字段提取（LangChain的Document对象包含metadata）\n",
    "    def convert_metadata(metadata):\n",
    "        for key, value in metadata.items():\n",
    "            if isinstance(value, datetime):\n",
    "                print('value: ',value)\n",
    "                metadata[key] = value.isoformat()\n",
    "        return metadata\n",
    "\n",
    "    texts = [doc.page_content for doc in chunks]\n",
    "    metadata = [convert_metadata(doc.metadata) for doc in chunks]\n",
    "    return texts, metadata\n",
    "# ====================\n",
    "# 5. 编码 文档，生成稠密和稀疏向量\n",
    "# ====================\n",
    "def embed_documents(texts, embedder):\n",
    "    embeddings = embedder.encode(\n",
    "        texts,\n",
    "        return_dense=True,\n",
    "        return_sparse=True,\n",
    "        return_colbert_vecs=False\n",
    "    )\n",
    "    dense_vecs = embeddings[\"dense_vecs\"]\n",
    "    sparse_vecs = embeddings[\"lexical_weights\"]\n",
    "    \n",
    "    print('Embeddings length: ', len(dense_vecs))\n",
    "    dim = len(dense_vecs[0])\n",
    "    print('dim: ',dim)\n",
    "    \n",
    "    return dense_vecs, sparse_vecs, dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./02 高考填志愿-大学各专业介绍.pdf\n",
      "data:  118\n",
      "batch_size:  32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting PDF into chunks: 100%|██████████| 4/4 [00:00<00:00, 699.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks:  118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings length:  118\n",
      "dim:  1024\n"
     ]
    }
   ],
   "source": [
    "pdf_dir = './pdf'\n",
    "# 编码器初始化\n",
    "embedder = init_embedding_model(model_dir=EMBEDDING_MODEL_PATH, devices=devices)\n",
    "texts, metadata = ingest_document(pdf_dir, batch_size=32)\n",
    "dence_vecs, sparse_vecs, dim = embed_documents(texts, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 6. Milvus向量数据库配置\n",
    "# ====================\n",
    "class VectorDB:\n",
    "    def __init__(self, collection_name=\"hybrid_search_collection\", dim=1024, index_params={\"index_type\": \"FLAT\", \"metric_type\": \"IP\"}, sparse_index_params={\"index_type\": \"SPARSE_INVERTED_INDEX\", \"metric_type\": \"IP\"}):\n",
    "        self.collection_name = collection_name\n",
    "        self.embedding_dim = dim  # 根据模型输出维度调整\n",
    "        self.index_params = index_params\n",
    "        self.sparse_index_params = sparse_index_params\n",
    "        \n",
    "        if not self._check_collection():\n",
    "            self._create_collection()\n",
    "            \n",
    "        self.collection = Collection(self.collection_name)\n",
    "    \n",
    "    def _check_collection(self):\n",
    "        return utility.has_collection(self.collection_name)\n",
    "    \n",
    "    def _create_collection(self):\n",
    "        fields = [\n",
    "            FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "            FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "            FieldSchema(name=\"sparse_vectors\", dtype=DataType.SPARSE_FLOAT_VECTOR),\n",
    "            FieldSchema(name=\"dense_vectors\", dtype=DataType.FLOAT_VECTOR, dim=self.embedding_dim),\n",
    "            FieldSchema(name=\"metadata\", dtype=DataType.JSON)\n",
    "        ]\n",
    "        \n",
    "        schema = CollectionSchema(fields, description=\"Document chunks with embeddings\")\n",
    "        self.collection = Collection(self.collection_name, schema, consistency_level=\"Strong\")\n",
    "        \n",
    "        self.collection.create_index(\"sparse_vectors\", self.sparse_index_params)\n",
    "        self.collection.create_index(\"dense_vectors\", self.index_params)\n",
    "        # # 在 _create_collection 中添加标量索引\n",
    "        # self.collection.create_index(\n",
    "        #     field_name=\"metadata\", \n",
    "        #     index_params={\n",
    "        #         \"index_type\": \"STL_SORT\",\n",
    "        #         \"metric_type\": \"JSON\"  # Milvus 2.3+ 支持 JSON 字段索引\n",
    "        #     }\n",
    "        # )\n",
    "    \n",
    "    def insert(self, text, sparse_vectors, dense_vectors, metadata):\n",
    "        \"\"\"存储文本块和嵌入向量\"\"\"\n",
    "        entities = [\n",
    "            text,  # text字段\n",
    "            sparse_vectors,  # sparse_vectors字段\n",
    "            dense_vectors,  # dense_vectors字段\n",
    "            metadata  # metadata字段\n",
    "        ]\n",
    "        self.collection.insert(entities)\n",
    "        self.collection.flush()\n",
    "    \n",
    "    def search(self, query_embedding, top_k=10, search_params={\"metric_type\": \"IP\"}, sparse_search_params={\"metric_type\": \"IP\"}):\n",
    "        self.collection.load()\n",
    "        \"\"\"相似性搜索\"\"\"\n",
    "        self.search_params = search_params\n",
    "        self.sparse_search_params = sparse_search_params\n",
    "        \n",
    "        sparse_req = AnnSearchRequest(query_embedding[\"sparse\"],\n",
    "                              \"sparse_vectors\", self.sparse_search_params, limit=top_k)\n",
    "        dense_req = AnnSearchRequest(query_embedding[\"dense\"],\n",
    "                             \"dense_vectors\", self.search_params, limit=top_k)\n",
    "        \n",
    "        results = self.collection.hybrid_search(\n",
    "            [sparse_req, dense_req],\n",
    "            rerank=RRFRanker(),\n",
    "            limit=top_k,\n",
    "            output_fields=[\"text\", \"metadata\"]\n",
    "        )\n",
    "\n",
    "        return [\n",
    "                    {\n",
    "                        \"text\": hit.entity.get('text'),\n",
    "                        \"metadata\": hit.entity.get('metadata'),\n",
    "                        \"distance\": hit.distance\n",
    "                    }\n",
    "                    for hit in results[0]\n",
    "                ] # [hit.entity.get('text') for hit in results[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(texts) < 1e4:\n",
    "index_params = {\n",
    "    \"index_type\": \"FLAT\",  # 无需参数配置\n",
    "    \"metric_type\": \"IP\"    # 根据需求选 L2/IP/COSINE\n",
    "    }\n",
    "sparse_index_params = {\n",
    "    \"index_type\": \"SPARSE_INVERTED_INDEX\", \n",
    "    \"metric_type\": \"IP\"\n",
    "    }\n",
    "search_params = {\"metric_type\": \"IP\"}\n",
    "sparse_search_params = {\"metric_type\": \"IP\"}\n",
    "# else:\n",
    "#     index_params = {\n",
    "#         \"index_type\": \"IVF_FLAT\",\n",
    "#         \"metric_type\": \"IP\",\n",
    "#         \"params\": {\n",
    "#             \"nlist\": min(256, int(np.sqrt(len(texts)))),  # N为数据总量\n",
    "#         }\n",
    "#     }\n",
    "#     sparse_index_params = {\n",
    "#         \"index_type\": \"SPARSE_INVERTED_INDEX\", \n",
    "#         \"metric_type\": \"IP\"\n",
    "#     }\n",
    "#     search_params = {\n",
    "#         \"metric_type\": \"IP\",\n",
    "#         \"params\": {\"nprobe\": index_params[\"params\"][\"nlist\"] // 10}\n",
    "#     }\n",
    "#     sparse_search_params = {\"metric_type\": \"IP\"}\n",
    "vector_db = VectorDB(collection_name=COLLECTION_NAME,dim=dim, index_params=index_params, sparse_index_params=sparse_index_params)\n",
    "vector_db.insert(texts, sparse_vecs, dence_vecs, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_context(question, embedder, vector_db, top_k=10, search_params={\"metric_type\": \"IP\"}, sparse_search_params={\"metric_type\": \"IP\"}):\n",
    "    \"\"\"检索上下文\"\"\"\n",
    "    # 生成问题嵌入\n",
    "    query_embedding_raw = embedder.encode(\n",
    "        [question], \n",
    "        return_dense=True, \n",
    "        return_sparse=True, \n",
    "        return_colbert_vecs=False)\n",
    "    \n",
    "    query_embedding = {}\n",
    "    query_embedding[\"dense\"] = query_embedding_raw[\"dense_vecs\"]\n",
    "    query_embedding[\"sparse\"] = [dict(query_embedding_raw[\"lexical_weights\"][0])]\n",
    "    \n",
    "    # 向量数据库检索\n",
    "    results = vector_db.search(query_embedding, top_k=top_k, search_params=search_params, sparse_search_params=sparse_search_params)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "问题： 导演的主要从事工作是？\n",
      "答案： 导演的主要从事工作包括：\n",
      "- MTV或广告导演\n",
      "- 电视剧导演\n",
      "- 电影导演\n",
      "- 舞台导演\n",
      "- 相应特定栏目后期工作人员\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "openai_api_key = \"token-abc123\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "# 进行问答\n",
    "while True:\n",
    "    query = input(\"\\n请输入问题（输入q退出）: \")\n",
    "    if query.lower() == 'q':\n",
    "        break\n",
    "    \n",
    "    results = search_context(query, embedder, vector_db,  top_k=10, search_params=search_params, sparse_search_params=sparse_search_params)\n",
    "    context_chunks = [result['text'] for result in results]\n",
    "    context = \"\\n\".join(context_chunks)\n",
    "    # 生成答案\n",
    "    prompt = f\"\"\"请仅根据给定上下文回答问题。\\n上下文：\\n{context}\\n问题：{query}\\n答案：\"\"\"\n",
    "    completion = client.chat.completions.create(model=\"/llm/Qwen2.5-7B-Instruct\", messages=[\n",
    "            {\"role\": \"system\", \"content\": \"请仅根据给定上下文回答问题。\"},\n",
    "            {\"role\": \"user\", \"content\": f\"上下文：\\n{context}\\n问题：{query}\\n答案：\"},\n",
    "        ]\n",
    "    )\n",
    "    anwser = completion.choices[0].message.content\n",
    "    print(\"=====================================\")\n",
    "    print(\"问题：\", query)\n",
    "    print(\"答案：\", anwser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 问答流程\n",
    "def ask(query, generator, results):\n",
    "    \"\"\"问答流程\"\"\"\n",
    "    context_chunks = [result['text'] for result in results]\n",
    "    context = \"\\n\".join(context_chunks)\n",
    "    \n",
    "    # 生成答案\n",
    "    prompt = f\"\"\"请仅根据给定上下文回答问题。\\n上下文：\\n{context}\\n问题：{query}\\n答案：\"\"\"\n",
    "    # instruction = \"给定上下文和问题，仅输出问题的答案.\"\n",
    "    # user_input_text = f\"上下文：\\n{context}\\n问题：{question}\\n答案：\"\n",
    "    # prompt = []\n",
    "    # prompt.append(\n",
    "    #             [\n",
    "    #                 {\"role\": \"system\", \"content\": instruction},\n",
    "    #                 {\"role\": \"user\", \"content\": user_input_text}\n",
    "    #             ]\n",
    "    #         )\n",
    "    response = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    \n",
    "    return response[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成器初始化\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"/disk3/lsp/HippoRAG/src/meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    device=device,\n",
    "    torch_dtype=torch.bfloat16,  # 使用bfloat16加速\n",
    ")\n",
    "\n",
    "# 进行问答\n",
    "while True:\n",
    "    question = input(\"\\n请输入问题（输入q退出）: \")\n",
    "    if question.lower() == 'q':\n",
    "        break\n",
    "    \n",
    "    results = search_context(question, embedder, vector_db, search_params, sparse_search_params)\n",
    "    answer = ask(question, generator, results)\n",
    "    print(f\"\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 5. 问答系统实现\n",
    "# ====================\n",
    "class QASystem:\n",
    "    def __init__(self):\n",
    "        self.embedder = init_embedding_model()\n",
    "        self.vector_db = VectorDB()\n",
    "        self.generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=\"/disk3/lsp/HippoRAG/src/meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            device=1  # 使用CPU，GPU可设为0\n",
    "        )\n",
    "    \n",
    "    def ingest_document(self, pdf_dir):\n",
    "        \"\"\"文档处理流水线\"\"\"\n",
    "        docs = extract_text_from_pdf(pdf_dir)\n",
    "        chunks = split_documents(docs)\n",
    "        print('chunks: ',len(chunks))\n",
    "        \n",
    "        # 在 ingest_document 中增加字段提取（LangChain的Document对象包含metadata）\n",
    "        def convert_metadata(metadata):\n",
    "            for key, value in metadata.items():\n",
    "                if isinstance(value, datetime):\n",
    "                    metadata[key] = value.isoformat()\n",
    "            return metadata\n",
    "        \n",
    "        texts = [doc.page_content for doc in chunks]\n",
    "        metadata = [convert_metadata(doc.metadata) for doc in chunks]\n",
    "        embeddings = self.embedder.embed_documents(texts)\n",
    "        print('dim: ',len(embeddings[0]))\n",
    "        self.vector_db.store_embeddings(texts, embeddings, metadata)\n",
    "    \n",
    "    def ask(self, question):\n",
    "        \"\"\"问答流程\"\"\"\n",
    "        # 生成问题嵌入\n",
    "        query_embedding = self.embedder.embed_query(question)\n",
    "        \n",
    "        # 向量数据库检索\n",
    "        results = self.vector_db.search(query_embedding)\n",
    "        context_chunks = [result['text'] for result in results]\n",
    "        context = \"\\n\".join(context_chunks)\n",
    "        \n",
    "        # 生成答案\n",
    "        prompt = f\"\"\"请仅根据给定上下文回答问题，仅输出问题的答案：\\n上下文：\\n{context}\\n问题：{question}\\n答案：\"\"\"\n",
    "        response = self.generator(\n",
    "            prompt,\n",
    "            max_new_tokens=100,\n",
    "            temperature=1.0,\n",
    "        )\n",
    "        \n",
    "        return response[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化系统\n",
    "qa_system = QASystem()\n",
    "\n",
    "# 文档处理（只需运行一次）\n",
    "qa_system.ingest_document(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 进行问答\n",
    "while True:\n",
    "    question = input(\"\\n请输入问题（输入q退出）: \")\n",
    "    if question.lower() == 'q':\n",
    "        break\n",
    "        \n",
    "    answer = qa_system.ask(question)\n",
    "    print(f\"\\n{answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "milvus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
